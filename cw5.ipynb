{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Zadanie 5\n",
        "\n",
        "\n",
        "Celem ćwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
        "\n",
        "Następnie należy wytrenować perceptron wielowarstwowy do klasyfikacji zbioru danych [MNIST](http://yann.lecun.com/exdb/mnist/). Zbiór MNIST dostępny jest w pakiecie `scikit-learn`.\n",
        "\n",
        "Punktacja:\n",
        "1. Implementacja propagacji do przodu (`forward`) [1 pkt]\n",
        "2. Implementacja wstecznej propagacji (zademonstrowana na bramce XOR) (`backward`) [2 pkt]\n",
        "3. Przeprowadzenie eksperymentów na zbiorze MNIST, w tym:\n",
        "    1. Porównanie co najmniej dwóch architektur sieci [1 pkt]\n",
        "    2. Przetestowanie każdej architektury na conajmniej 3 ziarnach [1 pkt]\n",
        "    3. Wnioski 1.[5 pkt]\n",
        "4. Jakość kodu 0.[5 pkt]\n",
        "\n",
        "Polecane źródła - teoria + intuicja:\n",
        "1. [Karpathy, CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1](https://www.youtube.com/watch?v=i94OvYb6noo&ab_channel=AndrejKarpathy)\n",
        "2. [3 Blude one Brown, Backpropagation calculus | Chapter 4, Deep learning\n",
        "](https://www.youtube.com/watch?v=tIeHLnjs5U8&t=4s&ab_channel=3Blue1Brown)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abc import abstractmethod, ABC\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ITER = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Layer(ABC):\n",
        "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._learning_rate = 0.01\n",
        "        # self.input = None\n",
        "        # self.output = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Forward propagation of x through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, output_error_derivative) ->np.ndarray:\n",
        "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    @learning_rate.setter\n",
        "    def learning_rate(self, learning_rate):\n",
        "        assert learning_rate < 1, f\"Given learning_rate={learning_rate} is larger than 1\"\n",
        "        assert learning_rate > 0, f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
        "        self._learning_rate = learning_rate\n",
        "\n",
        "class FullyConnected(Layer):\n",
        "    def __init__(self, input_size:int, output_size:int) -> None:\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.weights = np.random.randn(output_size, input_size)\n",
        "        self.biases = np.zeros((output_size, 1))\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        self.input = x\n",
        "        self.output = np.dot(self.weights, x) + self.biases\n",
        "        return self.output\n",
        "\n",
        "    # def backward(self, output_error_derivative)->np.ndarray:\n",
        "    #     input_error_derivative = np.dot(self.weights.T, output_error_derivative)\n",
        "    #     weights_error_derivative = np.dot(output_error_derivative, self.input.T)\n",
        "    #     self.weights -= self.learning_rate * weights_error_derivative\n",
        "    #     self.biases -= self.learning_rate * output_error_derivative\n",
        "    #     # input_error_derivative = np.dot(self.weights.T, output_error_derivative)\n",
        "    #     # self.weights_grad = np.dot(output_error_derivative, self.input.T)\n",
        "    #     # self.biases_grad = np.sum(output_error_derivative, axis=1, keepdims=True)\n",
        "    #     return input_error_derivative\n",
        "\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        input_error_derivative = np.dot(self.weights.T, output_error_derivative)\n",
        "        weights_error_derivative = np.dot(output_error_derivative, self.input.T)\n",
        "        self.weights -= self.learning_rate * weights_error_derivative\n",
        "        self.biases -= self.learning_rate * np.sum(output_error_derivative, axis=1, keepdims=True)\n",
        "        return input_error_derivative\n",
        "\n",
        "\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x:np.ndarray)->np.ndarray:\n",
        "        self.input = x\n",
        "        self.output = np.tanh(x)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_error_derivative)->np.ndarray:\n",
        "        return output_error_derivative * (1 - np.tanh(self.input) ** 2)\n",
        "    \n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.input = x\n",
        "        self.output = 1 / (1 + np.exp(-x))\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        return output_error_derivative * self.output * (1 - self.output)\n",
        "    \n",
        "class ReLU(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.input = x\n",
        "        self.output = np.maximum(0, x)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        return output_error_derivative * (self.input > 0).astype(int)\n",
        "\n",
        "\n",
        "class Loss:\n",
        "    def __init__(self, loss_function:callable, loss_function_derivative:callable)->None:\n",
        "        self.loss_function = loss_function\n",
        "        self.loss_function_derivative = loss_function_derivative\n",
        "\n",
        "    def loss(self, x:np.ndarray, y:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Loss function for a particular x\"\"\"\n",
        "        return self.loss_function(x, y)\n",
        "\n",
        "    def loss_derivative(self, x:np.ndarray, y:np.ndarray)->np.ndarray:\n",
        "        \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
        "        return self.loss_function_derivative(x, y)\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layers:List[Layer], learning_rate:float)->None:\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def compile(self, loss:Loss)->None:\n",
        "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
        "        self.loss = loss\n",
        "\n",
        "    def __call__(self, x:np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through all layers\"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def fit(self,\n",
        "            x_train:np.ndarray,\n",
        "            y_train:np.ndarray,\n",
        "            epochs:int,\n",
        "            learning_rate:float,\n",
        "            verbose:int=0)->None:\n",
        "        \"\"\"Fit the network to the training data\"\"\"\n",
        "        all_current_loss = []\n",
        "        epoch_loss = np.zeros(1437)\n",
        "        for layer in self.layers:\n",
        "            layer.learning_rate = self.learning_rate\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(len(x_train)):\n",
        "                # Forward propagation\n",
        "                # print(x_train[i].shape)\n",
        "                # x_train[i] = x_train[i].reshape((-1, 1))\n",
        "                output = self(x_train[i])\n",
        "                # if i==0:\n",
        "                    # print(output)\n",
        "                # Calculate loss and its derivative\n",
        "                correct_result = np.zeros((10, 1))\n",
        "                temp = y_train[i]\n",
        "                correct_result[y_train[i]] = 1\n",
        "                # if epoch == 99:\n",
        "                #     pass\n",
        "                # correct_result = y_train[i]\n",
        "                current_loss = self.loss.loss(output, correct_result)\n",
        "                loss_derivative = self.loss.loss_derivative(output, correct_result)\n",
        "                epoch_loss[i] = current_loss\n",
        "                # Backpropagation\n",
        "                for layer in reversed(self.layers):\n",
        "                    loss_derivative = layer.backward(loss_derivative)\n",
        "\n",
        "                # Update weights and biases\n",
        "                # for layer in self.layers:\n",
        "                #     if isinstance(layer, FullyConnected):\n",
        "                #         layer.weights -= layer.learning_rate * layer.weights_grad\n",
        "                #         layer.biases -= layer.learning_rate * layer.biases_grad\n",
        "            all_current_loss.append(np.mean(epoch_loss))\n",
        "            print(np.mean(epoch_loss))\n",
        "        fig2 = plt.plot(all_current_loss)\n",
        "        # plt.ylim([-10, 20])\n",
        "        # plt.xlabel('Epizod')\n",
        "        # plt.ylabel('Suma nagród')\n",
        "        # plt.title(f'Wykres sumy nagród uzyskanych przy rozwiązywaniu problemu\\nLiczba podejść = {testing_iter}')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "17.22754788218943\n",
            "4.2872368162190195\n",
            "1.4257205096216177\n",
            "0.7096831142039108\n",
            "0.3777881662967506\n",
            "0.2757091324537053\n",
            "0.21195079436872513\n",
            "0.18324438068356363\n",
            "0.16565066915375476\n",
            "0.155313709233785\n",
            "0.15020086532643384\n",
            "0.14240690812130385\n",
            "0.13703098921596402\n",
            "0.13195100334848964\n",
            "0.12537425189401957\n",
            "0.12277792903623991\n",
            "0.1208930856560534\n",
            "0.11954233341774698\n",
            "0.11851374477567725\n",
            "0.117169843934585\n",
            "0.11636108418451888\n",
            "0.11573486108477003\n",
            "0.11515913620269785\n",
            "0.11465963835577041\n",
            "0.11423306426402392\n",
            "0.11368891042163258\n",
            "0.11262409821726366\n",
            "0.11162547694447324\n",
            "0.1107005030805947\n",
            "0.1098495182581432\n",
            "0.10929085705383162\n",
            "0.10901935226529152\n",
            "0.10818056501880377\n",
            "0.10736487846055749\n",
            "0.10546428793835647\n",
            "0.10740000148631347\n",
            "0.10680832455687958\n",
            "0.10640569923144504\n",
            "0.10607458796767487\n",
            "0.10582043999815267\n",
            "0.10560345909109821\n",
            "0.10540420522179951\n",
            "0.10517044269021136\n",
            "0.10489632923801638\n",
            "0.10455777348535743\n",
            "0.1041684023705785\n",
            "0.1039001010598002\n",
            "0.10368818432931605\n",
            "0.10338613718222449\n",
            "0.10134663648873192\n",
            "0.10132971317980534\n",
            "0.10123173609533412\n",
            "0.10113003905991158\n",
            "0.10103609693590923\n",
            "0.10094045341440644\n",
            "0.10084601554536518\n",
            "0.1007421430617854\n",
            "0.10063180786292589\n",
            "0.10051898255973828\n",
            "0.10039637178460314\n",
            "0.10022567152009079\n",
            "0.09995241411970614\n",
            "0.0997936928527958\n",
            "0.09969221647693949\n",
            "0.09956054182996586\n",
            "0.09912481130179311\n",
            "0.09867769133215747\n",
            "0.09857008459393042\n",
            "0.09845115604637165\n",
            "0.09833053221825137\n",
            "0.09818641243412624\n",
            "0.09804767288925621\n",
            "0.09788477253913559\n",
            "0.09771608843472566\n",
            "0.09753121845274339\n",
            "0.09733239938246331\n",
            "0.09710661278463222\n",
            "0.09678076670284695\n",
            "0.09668866476035845\n",
            "0.09643153725881451\n",
            "0.09639669788722074\n",
            "0.09628029459605571\n",
            "0.09617670875051101\n",
            "0.09609041387002855\n",
            "0.09601256955012841\n",
            "0.09593929684621429\n",
            "0.09586614409610172\n",
            "0.09585888031407616\n",
            "0.09583419126328713\n",
            "0.09578210437715182\n",
            "0.09569451931573797\n",
            "0.0956557576063472\n",
            "0.0955908381282724\n",
            "0.09556773847504034\n",
            "0.09549340287947622\n",
            "0.0954681954400944\n",
            "0.09541153624542026\n",
            "0.09539715992622444\n",
            "0.09532833818433217\n",
            "0.09530771919082252\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "360\n",
            "Accuracy on test set: 0.1111111111111111\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Załadowanie zbioru danych cyfr MNIST\n",
        "digits = load_digits()\n",
        "\n",
        "# Przygotowanie danych\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# Standaryzacja danych\n",
        "# scaler = StandardScaler()\n",
        "# X = scaler.fit_transform(X)\n",
        "\n",
        "# Podział danych na zbiór treningowy i testowy\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "num_classes = 10  # Dla MNIST cyfr mamy 10 klas (cyfry od 0 do 9)\n",
        "\n",
        "\n",
        "\n",
        "# Inicjalizacja sieci\n",
        "layers = [\n",
        "    FullyConnected(input_size=X_train.shape[1], output_size=128),  # Warstwa wejściowa\n",
        "    Tanh(),  # Funkcja aktywacji\n",
        "    FullyConnected(input_size=128, output_size=64),  # Ukryta warstwa\n",
        "    Tanh(),  # Funkcja aktywacji\n",
        "    FullyConnected(input_size=64, output_size=num_classes),  # Warstwa wyjściowa\n",
        "    # Tanh()\n",
        "]\n",
        "\n",
        "# layers = [\n",
        "#     FullyConnected(input_size=X_train.shape[1], output_size=128),  # Warstwa wejściowa\n",
        "#     Tanh(),  # Funkcja aktywacji\n",
        "#     FullyConnected(input_size=128, output_size=num_classes),  # Warstwa wyjściowa\n",
        "#     # Tanh()\n",
        "# ]\n",
        "\n",
        "# layers = [\n",
        "#     FullyConnected(input_size=X_train.shape[1], output_size=128),  # Warstwa wejściowa\n",
        "#     ReLU(),  # Funkcja aktywacji\n",
        "#     FullyConnected(input_size=128, output_size=64),  # Ukryta warstwa\n",
        "#     ReLU(),  # Funkcja aktywacji\n",
        "#     FullyConnected(input_size=64, output_size=num_classes),  # Warstwa wyjściowa\n",
        "#     ReLU()\n",
        "# ]\n",
        "\n",
        "learning_rate = 0.0001\n",
        "network = Network(layers=layers, learning_rate=learning_rate)\n",
        "\n",
        "# Kompilacja sieci z funkcją straty\n",
        "loss = Loss(loss_function=lambda x, y: np.mean((x - y) ** 2),\n",
        "            loss_function_derivative=lambda x, y: 2 * (x - y))\n",
        "network.compile(loss=loss)\n",
        "\n",
        "# Uczenie sieci\n",
        "epochs = 100\n",
        "\n",
        "new_X_train = np.empty((1437, 64, 1))\n",
        "y_train_one_hot = np.eye(10)[y_train]\n",
        "print(y_train_one_hot[1])\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    temp = X_train[i]\n",
        "    temp = temp.reshape((64, 1))\n",
        "    new_X_train[i] = temp\n",
        "    pass\n",
        "\n",
        "network.fit(new_X_train, y_train, epochs, learning_rate)\n",
        "\n",
        "# Ocena na danych testowych\n",
        "predictions = []\n",
        "new_X_test = np.empty((1437, 64, 1))\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    temp = X_test[i]\n",
        "    temp = temp.reshape((64, 1))\n",
        "    new_X_test[i] = temp\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    output = network(new_X_test[i])\n",
        "    prediction = np.argmax(output)\n",
        "    predictions.append(prediction)\n",
        "print(len(predictions))\n",
        "accuracy = np.mean(np.array(predictions) == y_test)\n",
        "print(f\"Accuracy on test set: {accuracy}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Eksperymenty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Wnioski"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
