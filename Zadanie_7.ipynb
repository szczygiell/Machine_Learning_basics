{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpar5LziY_-0"
      },
      "source": [
        "#Zadanie 7 (7 pkt)\n",
        "Celem zadania jest zaimplementowanie dwóch wersji naiwnego klasyfikatora Bayesa.\n",
        "* W pierwszej wersji należy dokonać dyskretyzacji danych - przedział wartości każdego atrybutu dzielimy na cztery równe przedziały i każdej ciągłej wartości atrybutu przypisujemy wartość dyskretną wynikająca z przynależności do danego przedziału.\n",
        "* W drugiej wersji wartości likelihood wyliczamy z rozkładów normalnych o średnich i odchyleniach standardowych wynikających z wartości atrybutów.\n",
        "Trening i test należy przeprowadzić dla zbioru Iris, tak jak w przypadku zadania z drzewem klasyfikacyjnym. Proszę przeprowadzić eksperymenty najpierw dla DOKŁADNIE takiego podziału zbioru testowego i treningowego jak umieszczony poniżej. W dalszej części należy przeprowadzić analizę działania klasyfikatorów dla różnych wartości parametrów. Proszę korzystać z przygotowanego szkieletu programu, oczywiście można go modyfikować według potrzeb. Wszelkie elementy szkieletu zostaną wyjaśnione na zajęciach.\n",
        "\n",
        "* Dyskretyzacja danych - **0.5 pkt**\n",
        "* Implementacja funkcji rozkładu normalnego o zadanej średniej i odchyleniu standardowym. - **0.5 pkt**\n",
        "* Implementacja naiwnego klasyfikatora Bayesa dla danych dyskretnych. - **2.0 pkt**\n",
        "* Implementacja naiwnego klasyfikatora Bayesa dla danych ciągłych. - **2.5 pkt**\n",
        "* Przeprowadzenie eksperymentów, wnioski i sposób ich prezentacji. - **1.5 pkt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "XNc-O3npA-J9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "fBh2tfQ44u5k"
      },
      "outputs": [],
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.priors = {}\n",
        "        self.likelihoods = {}\n",
        "\n",
        "    def build_classifier(self, train_features, train_classes):\n",
        "        #  a priori P(A)\n",
        "        class_counter =Counter(train_classes)\n",
        "        self.priors = dict(class_counter)\n",
        "        total_samples = len(train_classes)\n",
        "        for key in self.priors:\n",
        "            self.priors[key] /= total_samples\n",
        "\n",
        "        # prawdopodobieństwo P(B|A)\n",
        "        p = np.zeros((len(set(train_classes)), train_features.shape[1], 4))\n",
        "        for class_elem, feature in zip(train_classes, train_features):     \n",
        "            j=0\n",
        "            for value in feature:\n",
        "                p[int(class_elem), j, int(value)] += 1\n",
        "                j += 1\n",
        "\n",
        "        # przygotowanie słownika\n",
        "        self.likelihoods = {key: 1 for key in set(train_classes)}\n",
        "        for key in self.likelihoods.keys():\n",
        "            self.likelihoods[key] = {key2: 1 for key2 in range(len(train_features[0]))}\n",
        "            for key2 in self.likelihoods[key].keys():\n",
        "                self.likelihoods[key][key2] = {key3: 1 for key3 in range(len(set(train_features.flatten())))}\n",
        "\n",
        "        \n",
        "        for class_val in class_counter:\n",
        "            for feature in train_features:       \n",
        "                for j in range(len(feature)):\n",
        "                    self.likelihoods[class_val][j][feature[j]] *= p[class_val, j, int(feature[j])] / class_counter[class_val]\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def data_discretization(data:np.ndarray):\n",
        "        for i in range(data.shape[1]):\n",
        "            bin_edges = np.linspace(data[:, i].min(), data[:, i].max(), 4)\n",
        "            data[:, i] = np.digitize(data[:, i], bins=bin_edges, right=True)\n",
        "        return data\n",
        "\n",
        "    def predict(self, sample):\n",
        "        posteriors = {}\n",
        "\n",
        "        # posteriory P(A|B)\n",
        "        for class_val, class_prob in self.priors.items():\n",
        "            posterior = class_prob #class_prob -> a priori\n",
        "            for i in range(len(sample)):\n",
        "                posterior *= self.likelihoods[class_val][i][sample[i]]\n",
        "\n",
        "            posteriors[class_val] = posterior\n",
        "        return max(posteriors, key=posteriors.get)\n",
        "\n",
        "\n",
        "class GaussianNaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.priors = {}\n",
        "        self.likelihoods = {}\n",
        "\n",
        "    def build_classifier(self, train_features, train_classes):\n",
        "        #  a priori P(A)\n",
        "        self.priors = dict(Counter(train_classes))\n",
        "        total_samples = len(train_classes)\n",
        "        for key in self.priors:\n",
        "            self.priors[key] /= total_samples\n",
        "\n",
        "        # prawdopodobieństwo P(B|A)\n",
        "        for class_val in set(train_classes):\n",
        "            class_indices = np.where(train_classes == class_val)\n",
        "            class_data = train_features[class_indices]\n",
        "\n",
        "            means = np.mean(class_data, axis=0)\n",
        "            stds = np.std(class_data, axis=0)\n",
        "\n",
        "            self.likelihoods[class_val] = {'mean': means, 'std': stds}\n",
        "\n",
        "    @staticmethod\n",
        "    def normal_dist(x, mean, std):\n",
        "        exponent = np.exp(-(np.power(x - mean, 2) / (2 * np.power(std, 2))))\n",
        "        return (1 / (np.sqrt(2 * np.pi) * std)) * exponent\n",
        "\n",
        "    def predict(self, sample):\n",
        "        posteriors = {}\n",
        "\n",
        "        for class_val, class_prob in self.priors.items():\n",
        "            posterior = class_prob\n",
        "            for i in range(len(sample)):\n",
        "                posterior *= GaussianNaiveBayes.normal_dist(sample[i],\n",
        "                                                              self.likelihoods[class_val]['mean'][i],\n",
        "                                                              self.likelihoods[class_val]['std'][i])\n",
        "            posteriors[class_val] = posterior\n",
        "        return max(posteriors, key=posteriors.get)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        11\n",
            "           1       0.88      1.00      0.93         7\n",
            "           2       1.00      0.92      0.96        12\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.96      0.97      0.96        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=153)\n",
        "\n",
        "nb = NaiveBayes()\n",
        "\n",
        "x_train_discrete = NaiveBayes.data_discretization(x_train)\n",
        "\n",
        "nb.build_classifier(x_train_discrete, y_train)\n",
        "\n",
        "x_test_discrete = NaiveBayes.data_discretization(x_test)\n",
        "\n",
        "predictions = [nb.predict(sample) for sample in x_test_discrete]\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(y_test, predictions)}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       0.86      1.00      0.92         6\n",
            "           2       1.00      0.91      0.95        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.95      0.97      0.96        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        }
      ],
      "source": [
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=123)\n",
        "\n",
        "nb = GaussianNaiveBayes()\n",
        "\n",
        "nb.build_classifier(x_train, y_train)\n",
        "\n",
        "predictions = [nb.predict(sample) for sample in x_test]\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(y_test, predictions)}')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, predictions))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
